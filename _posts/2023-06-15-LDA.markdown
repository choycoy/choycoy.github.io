---
title: "LDA(Latent Dirichlet Allocation)"
tags:
  - Topic Modelling
---

ğŸ’¡ This post introduces LDA(Latent Dirichlet Allocation).
{: .notice--warning}

Topic Modellingì´ë€ ë¬¸ì„œì˜ ì§‘í•©ì—ì„œ topicì„ ì°¾ì•„ë‚´ëŠ” ê³¼ì •ì´ë‹¤.
<br>
LDAëŠ” ì£¼ì–´ì§„ ë¬¸ì„œì—ì„œ ì–´ë–¤ ì£¼ì œë“¤ì´ ì¡´ì¬í•˜ëŠ”ì§€ë¥¼ í™•ë¥ ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ëŠ” í† í”½ ëª¨ë¸ë§ ê¸°ë²•ë“¤ ì¤‘ í•˜ë‚˜ì´ë‹¤.
<br>
<br>
# 1. Load dataset ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
e.g.
```
columns_name=['user_id','stars', 'item_id', 'text']
df = pd.read_csv("/content/drive/My Drive/final/example_texts_10000.csv", sep=",", names=columns_name, encoding='latin-1')
```
<br>
![df](https://github.com/choycoy/choycoy.github.io/assets/40441643/83d9c1f8-e1af-4ad9-a3ca-ac611b790d5b)
<br>
# 2. Preprocess data ë°ì´í„° ì „ì²˜ë¦¬í•˜ê¸°
## 1. text tokenisation í…ìŠ¤íŠ¸ í† í°í™”í•˜ê¸°
```
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import string

nltk.download('stopwords')
nltk.download('wordnet')

# Define the preprocessing function
def preprocess_text(text):
    # Tokenize the text
    tokens = text.lower().split()
```
## 2. Remove stopwords ë¶ˆìš©ì–´ ì œê±°í•˜ê¸°
```
stop_words = set(stopwords.words('english'))
tokens = [token for token in tokens if token not in stop_words]
```
## 3. Remove punctuations êµ¬ë‘ì  ì œê±°í•˜ê¸°
```
tokens = [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens]
```
## 4. Lemmatise the tokens í† í° í‘œì œì–´ ì¶”ì¶œí•˜ê¸°
```
lemmatizer = WordNetLemmatizer()
tokens = [lemmatizer.lemmatize(token) for token in tokens]
```
